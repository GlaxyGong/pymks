
Filter Example
==============

This example deomonstrates the connection between MKS and signal
processing for a 1D filter. It shows that the filter is in fact the same
as the influence coefficients and, thus, applying the ``predict`` method
of the ``MKSRegressionModel`` is the same as filtering the
microstructure.

.. code:: python

    %matplotlib inline
    %load_ext autoreload
    %autoreload 2
    
    import numpy as np
    import matplotlib.pyplot as plt

.. parsed-literal::

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload


The filter
----------

Here we construct a filter, :math:`F`, such that

.. math:: F\left(x\right) = e^{|x|} \cos{\left(2\pi x\right)} 

We want to show that if :math:`F` is used to generate sample calibration
data for the MKS, then the calculated influence coefficents are in fact
just :math:`F`.

.. code:: python

    x = np.linspace(-10,10, 1000)
    def F(x):
        return np.exp(-abs(x)) * np.cos(2 * np.pi * x)
    p = plt.plot(x, F(x))


.. image:: filter_files/filter_3_0.png


Next we generate the sample data ``(X, y)`` using
``scipy.ndimage.convolve``.

.. code:: python

    import scipy.ndimage
    
    Nspace = 101
    np.random.seed(201)
    x = np.linspace(-10, 10, Nspace)
    X = np.random.random((50, Nspace))
    y = np.array([scipy.ndimage.convolve(xx, F(x), mode='wrap') for xx in X])
For this problem, a basis is unnessary as no discretaztion is required
to accuraletly reproduce the convolution with the MKS. Using the
``ContinuousIndicatorBasis`` with ``n_states=2`` is the equivalent of
non-discretized convolution in space.

.. code:: python

    from pymks import MKSRegressionModel
    from pymks import ContinuousIndicatorBasis
    
    basis = ContinuousIndicatorBasis(n_states=2, domain=[0, 1])
    model = MKSRegressionModel(basis=basis)
Fit the model using the data generated by :math:`F`.

.. code:: python

    model.fit(X, y)
To check for internal consistency, we can compare the predicited output
with the original for a few values

.. code:: python

    y_pred = model.predict(X)
    print y[0, :4]
    print y_pred[0, :4]

.. parsed-literal::

    [-0.41059557  0.20004566  0.61200171  0.5878077 ]
    [-0.41059557  0.20004566  0.61200171  0.5878077 ]


With a slight linear manipulation of the coefficients, they agree
perfectly with filter :math:`F`.

.. code:: python

    p = plt.plot(x, F(x), 'r')
    p = plt.plot(x, -model.coeff[:,0] + model.coeff[:, 1], 'k--')


.. image:: filter_files/filter_13_0.png



Some manipulation of the coefficients is required to reproduce tht
filter

The convolution is written as,

.. math::  p \left[i\right] = \sum_h^{n-1} \alpha_h \left[j\right] m_h \left[i + j\right] 

The calculated influence coefficients are not even close to the
originals. What's going on? The equation,

.. math::  \sum\limits_{h=0}^{H-1} m^h_{a, s} = 1 

is not being taken into account. Using this, we can rewrite the
convolution as

.. math::

    \begin{split}
   p_{a,s} &= \sum\limits_{h=0}^{H - 1} \sum\limits_{t=0}^{S-1} \alpha_t^h m_{a,s + t}^h \\
           &= \sum\limits_{t=0}^{S-1} \left[ \sum\limits_{h=0}^{H - 2} \alpha_t^h m_{a,s + t}^h + 
              \alpha_t^{H-1} \left( 1 - \sum\limits_{h=0}^{H - 2} m_{a,s+t}^h \right) \right] \\
           &= \sum\limits_{t=0}^{S-1} \alpha_t^{H - 1} +
              \sum\limits_{h=0}^{H - 2} \sum\limits_{t=0}^{S-1} \left(\alpha_t^h - \alpha_t^{H-1} \right) m_{a,s + t}^h \\
           &= b_0 + \sum\limits_{h=0}^{H - 2} \sum\limits_{t=0}^{S-1} b_t^h m_{a,s + t}^h
   \end{split}

This removes the redundancies from the regression. Coding this in 1D is
easy, we simply replace

.. code:: python

    for k in range(Nspace):
        model_Fcoeff[k] = np.linalg.lstsq(X_[:,k], y[:,k] )[0]

with

.. code:: python

    for k in range(Nspace):
        if k == 0:
            model_Fcoeff[k] = np.linalg.lstsq(X_[:,k], y[:,k] )[0]
        else:
            model_Fcoeff[k,:-1] = np.linalg.lstsq(X_[:,k,:-1], y[:,k] )[0]


Introducing the ``MKSRegressionModel``
--------------------------------------

The ``MKSRegressionModel`` takes a test microstructure ``X`` and a test
response ``y`` in any dimension and does a linear regression to
determine the influence coefficients. So,

.. code:: python

    model = MKSRegressionModel(Nbin=10)
    model.fit(X, y)

After the fit, the ``predict`` method can be used to fit new data.

.. code:: python

    y_predict = model.predict(X_predict)

The ``MKSRegressionModel`` inherits from Scikit-learn's
``LinearRegression`` class so that we can start doing cross validation
in the next tutorial.

.. code:: python

    from pymks import MKSRegressionModel
    
    ??MKSRegressionModel
.. code:: python

    model = MKSRegressionModel(Nbin=2)
    model.fit(X, y)
.. code:: python

    model.coeff = np.fft.ifft(model.Fcoeff, axis=0)
    model.coeff = -(model.coeff - model.coeff[0, 1])[:,::-1]
    for b in range(Nbin):
        plt.figure()
        plt.plot(model.coeff[:,b], label='MKSRegressionModel')
        plt.plot(coeff[:,b], label='original')
        plt.legend()


.. image:: filter_files/filter_20_0.png



.. image:: filter_files/filter_20_1.png


The ``model.coeff`` needed rearranging as the ``MKSRegressionModel``
coefficients are using the coefficients that have the redundancies
removed.

Exercise 03-1
-------------

Create a random sample and check that using ``scipy.ndimage.convolve``
to create the responses gives the same result as the
``MKSRegressionModel``.

-  Use ``np.roll`` to align the result from ``scipy.ndimage.convolve``
-  Use ``np.allclose`` to compare.
